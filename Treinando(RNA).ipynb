import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
import shap
import lime
from lime.lime_tabular import LimeTabularExplainer

# 1. Carregar dataset
def load_data():
    url = "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data"
    columns = ['ID', 'Diagnosis'] + [f'Feature_{i}' for i in range(1, 31)]
    df = pd.read_csv(url, header=None, names=columns)
    df.drop(columns=['ID'], inplace=True)  # Remover ID
    return df

df = load_data()
print("\nüîç Visualiza√ß√£o inicial do dataset carregado:")
print(df.head())

# 2. Pr√©-processamento
def preprocess_data(df):
    label_encoder = LabelEncoder()
    df['Diagnosis'] = label_encoder.fit_transform(df['Diagnosis'])  # M -> 1, B -> 0
    X = df.drop(columns=['Diagnosis'])
    y = df['Diagnosis']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #Divis√£o entre treino e teste (80% treino, 20% teste)
    scaler = StandardScaler() #--
    X_train = scaler.fit_transform(X_train)# transforma os dados para que tenham m√©dia 0 e desvio padr√£o 1
    X_test = scaler.transform(X_test)#--
    return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = preprocess_data(df)
print("\n‚úÖ Dados normalizados e divididos em treino e teste.")

# 3. Modelos
def create_mlp():
    model = Sequential([
        Dense(64, activation='relu', input_shape=(X_train.shape[1],)), #64 neur√¥nios com ReLU, recebe as 30 features do dataset.
        Dense(32, activation='relu'),#desativa aleatoriamente 30% dos neur√¥nios a cada passo para evitar overfitting
        Dense(1, activation='sigmoid')#1 neur√¥nio com fun√ß√£o sigmoide
    ])
    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy']) 
    return model

print("\n‚öôÔ∏è  Iniciando o treinamento da Rede Neural (MLP)...")
mlp_model = create_mlp()
history = mlp_model.fit(X_train, y_train, epochs=20, validation_split=0.2, batch_size=16, verbose=1)
#epochs=20: passa pelos dados 20 vezes.
#validation_split=0.2: separa 20% do treino para valida√ß√£o interna.
#batch_size=16: treina com 16 amostras por vez (mais est√°vel que 1).

# Gr√°fico de Acur√°cia do MLP
print("\nüìà Gr√°fico de evolu√ß√£o da acur√°cia durante o treinamento da MLP:")
plt.figure(figsize=(10, 5))
plt.plot(history.history['accuracy'], label='Acur√°cia Treino', marker='o')
plt.plot(history.history['val_accuracy'], label='Acur√°cia Valida√ß√£o', marker='s')
plt.title('Evolu√ß√£o da Acur√°cia - MLP', fontsize=14)
plt.xlabel('√âpocas')
plt.ylabel('Acur√°cia')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# 4. Outros Modelos
print("\n‚öôÔ∏è  Treinando modelos de compara√ß√£o (Random Forest e SVM)...")
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
svm_model = SVC(kernel='rbf', probability=True, random_state=42)
svm_model.fit(X_train, y_train)

# 5. Avalia√ß√£o
def evaluate_model(model, X_test, y_test, model_name="Model"):
    print(f"\nüìä Avalia√ß√£o do modelo: {model_name}")
    y_pred = (model.predict(X_test) > 0.5).astype(int) if isinstance(model, tf.keras.Model) else model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    cm = confusion_matrix(y_test, y_pred)
    print(f"\n{model_name} Accuracy: {acc:.4f}")
    print(classification_report(y_test, y_pred))
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Purples', cbar=False, xticklabels=['Benigno', 'Maligno'], yticklabels=['Benigno', 'Maligno'])
    plt.title(f'Matriz de Confus√£o - {model_name}')
    plt.xlabel('Previsto')
    plt.ylabel('Real')
    plt.tight_layout()
    plt.show()

evaluate_model(mlp_model, X_test, y_test, "MLP")
evaluate_model(rf_model, X_test, y_test, "Random Forest")
evaluate_model(svm_model, X_test, y_test, "SVM")

# 6. Interpreta√ß√£o com SHAP
print("\nüß† An√°lise com SHAP: import√¢ncia das features no modelo Random Forest")
explainer = shap.Explainer(rf_model, X_train)
shap_values = explainer(X_test[:50])
shap.summary_plot(shap_values, X_test[:50])

# 7. Interpreta√ß√£o com LIME
print("\nüß™ An√°lise local com LIME: explica√ß√£o de uma predi√ß√£o espec√≠fica")
explainer = LimeTabularExplainer(X_train, mode="classification", training_labels=y_train, feature_names=df.columns[1:])
exp = explainer.explain_instance(X_test[0], rf_model.predict_proba, num_features=5)

# Salvar explica√ß√£o LIME como HTML
exp.save_to_file('explicacao_lime.html')
print("‚úÖ Explica√ß√£o LIME salva como 'explicacao_lime.html'. Abra o arquivo para ver a an√°lise visual.")
